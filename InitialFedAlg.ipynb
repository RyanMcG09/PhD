{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ded2b11",
   "metadata": {},
   "source": [
    "# Initial Federated Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f34b9",
   "metadata": {},
   "source": [
    "This script creates a deep learning model for the detection of cancer given a number of features, this is then compared to a federated learning model. In the interest of fairness, the initial model is trained on the same amount of data as a single node in the federated learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c59dc",
   "metadata": {},
   "source": [
    "Importing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a57d6",
   "metadata": {},
   "source": [
    "Activation and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda7ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function\n",
    "def sigmoid(v):\n",
    "    return 1/(1+np.exp(-v))\n",
    "def sigmoid_der(v):\n",
    "    return sigmoid(v)*(1-sigmoid(v))\n",
    "\n",
    "#Loss Function\n",
    "def crossEntrop(o,y):\n",
    "    return (-y*(np.log(o)) - (1-y)* np.log(1-o))\n",
    "def crossEntrDeriv(o,y):\n",
    "    return -(y/o - (1-y)/(1-o))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc214611",
   "metadata": {},
   "source": [
    "Reading data and converting to two training and testing nodes for experimentation. We use the MinMaxScaler preprocessing to prevent overflow later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1740975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Mixcancer.csv\")\n",
    "df = df.values\n",
    "\n",
    "df[:,1:] = preprocessing.MinMaxScaler().fit_transform(df[:,1:])\n",
    "\n",
    "zeros = df[df[:,0] == 0]\n",
    "ones = df[df[:,0] == 1]\n",
    "\n",
    "no0sper = int(zeros.shape[0]/2)\n",
    "no1sper = int(ones.shape[0]/2)\n",
    "\n",
    "node1 = np.concatenate([zeros[:int(0.4*zeros.shape[0]),:], ones[:int(0.4*ones.shape[0]),:]])\n",
    "node2 = np.concatenate([zeros[int(0.4*zeros.shape[0]):int(0.8*zeros.shape[0]),:], ones[int(0.4*ones.shape[0]):int(0.8*ones.shape[0]),:]])\n",
    "test1 = np.concatenate([zeros[int(0.8*zeros.shape[0]):int(0.9*zeros.shape[0]),:], ones[int(0.8*ones.shape[0]):int(0.9*ones.shape[0]),:]])\n",
    "test2 = np.concatenate([zeros[int(0.9*zeros.shape[0]):,:], ones[int(0.9*ones.shape[0]):,:]])\n",
    "fullTrain = np.concatenate([zeros[:int(0.8*zeros.shape[0]),:], ones[:int(0.8*ones.shape[0]),:]])\n",
    "fullTest = np.concatenate([zeros[int(0.8*zeros.shape[0]):,:], ones[int(0.8*ones.shape[0]):,:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a0de2",
   "metadata": {},
   "source": [
    "Splitting the nodes training and test data into features and target feature. And reshaping acitvation feature for use in DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82541166",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = node1[:,1:]\n",
    "y_train1 = node1[:,0]\n",
    "x_train2 = node2[:,1:]\n",
    "y_train2 = node2[:,0]\n",
    "\n",
    "x_test1 = test1[:,1:]\n",
    "y_test1 = test1[:,0]\n",
    "x_test2 = test2[:,1:]\n",
    "y_test2 = test2[:,0]\n",
    "\n",
    "x_train = fullTrain[:,1:]\n",
    "y_train = fullTrain[:,0]\n",
    "x_test = fullTest[:,1:]\n",
    "y_test = fullTest[:,0]\n",
    "\n",
    "def reshape(data):\n",
    "    data = data.reshape(len(data),1)\n",
    "    return data\n",
    "\n",
    "y_train = reshape(y_train)\n",
    "y_test = reshape(y_test)\n",
    "y_train1 = reshape(y_train1)\n",
    "y_test1 = reshape(y_test1)\n",
    "y_train2 = reshape(y_train2)\n",
    "y_test2 = reshape(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Altering the DNN algorithm to train two individual models\n",
    "def DNN(x_train,y_train):\n",
    "    np.random.seed(42)\n",
    "    neuronNo = 5\n",
    "    w1 = np.random.uniform(-1,1,[len(x_train[0]),neuronNo])\n",
    "    w2 = np.random.uniform(-1,1,[neuronNo,1])\n",
    "    b1 = np.zeros([1,neuronNo])\n",
    "    b2 = np.zeros([1,1])\n",
    "    l = 0.01\n",
    "    epochs = 1\n",
    "    miniBatch = 400\n",
    "\n",
    "    train_L = []\n",
    "    test_L = []\n",
    "    train_Acc = []\n",
    "    test_Acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(x_train[0]), miniBatch):\n",
    "\n",
    "            x_trainSample = x_train[i:i+miniBatch,:]\n",
    "            y_trainSample = y_train[i:i+miniBatch,:]\n",
    "\n",
    "            #feedforward\n",
    "            in1 = x_trainSample@w1 + b1\n",
    "            o1 = sigmoid(in1)\n",
    "            in2 = o1@w2 + b2\n",
    "            o2 = sigmoid(in2)\n",
    "\n",
    "            #backpropagation output layer\n",
    "            dE_dO2 = crossEntrDeriv(o2, y_trainSample)\n",
    "            dO2_dIn2 = sigmoid_der(in2)\n",
    "            dIn2_dW2 = o1\n",
    "            dIn2_B2 = 1\n",
    "            dE_dW2 = (1/miniBatch)*dIn2_dW2.T@(dE_dO2*dO2_dIn2)\n",
    "            dE_dB2 = (1/miniBatch)*np.ones([1,len(x_trainSample)])@(dE_dO2*dO2_dIn2)\n",
    "\n",
    "            #backpropagation hidden layer\n",
    "            dIn2_dO1 = w2\n",
    "            dO1_dIn1 = sigmoid_der(in1)\n",
    "            dIn1_dW1 = x_trainSample\n",
    "            dE_dW1 = (1/miniBatch)*dIn1_dW1.T@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "            dE_dB1 = (1/miniBatch)*np.ones([len(x_trainSample)])@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "\n",
    "            #updating parameters\n",
    "            b2-=l*dE_dB2\n",
    "            w2-=l*dE_dW2\n",
    "            b1-=l*dE_dB1\n",
    "            w1-=l*dE_dW1\n",
    "\n",
    "        #Error\n",
    "        error = crossEntrop(o2 ,y_trainSample).mean()\n",
    "        train_L.append(error)\n",
    "\n",
    "        #Accuracy\n",
    "        pred_train = np.where(o2 > 0.5, 1,0)\n",
    "        train_Acc.append(metrics.accuracy_score(y_trainSample,pred_train))\n",
    "\n",
    "    #print(\"Training: Loss: {0}. Accuracy: {1}. Error: {2}\".format(train_L[-1],train_Acc[-1], 1 - train_Acc[-1]))\n",
    "    print(\"Training: Loss: {0}. Accuracy: {1}\".format(train_L[-1],train_Acc[-1]))\n",
    "    \n",
    "    return w1,w2,b1,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd53544",
   "metadata": {},
   "source": [
    "Training The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7facecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Client One\n",
      "Training: Loss: 0.7145093617748137. Accuracy: 0.44\n",
      "Training on Client Two\n",
      "Training: Loss: 0.6985157381751381. Accuracy: 0.505\n",
      "Training on all data\n",
      "Training: Loss: 0.7065125499749757. Accuracy: 0.4725\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on Client One\")\n",
    "nodeOneW1,nodeOneW2,nodeOneB1,nodeOneB2 = DNN(x_train1,y_train1)\n",
    "print(\"Training on Client Two\")\n",
    "nodeTwoW1,nodeTwoW2,nodeTwoB1,nodeTwoB2 = DNN(x_train2,y_train2)\n",
    "print(\"Training on all data\")\n",
    "totalW1,totalW2,totalB1,totalB2 = DNN(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a07eb",
   "metadata": {},
   "source": [
    "We now have the weights and biases of two models trained individually, lets perform aggregation and see how our new model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccada435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleFedAvg():\n",
    "    globalW1 = (nodeOneW1 + nodeOneW1)/2\n",
    "    globalW2 = (nodeOneW2 + nodeOneW2)/2\n",
    "    globalB1 = (nodeOneB1 + nodeOneB1)/2\n",
    "    globalB2 = (nodeOneB2 + nodeOneB2)/2\n",
    "    return globalW1, globalW2, globalB1, globalB2\n",
    "    \n",
    "globalW1, globalW2, globalB1, globalB2 = simpleFedAvg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0bea8",
   "metadata": {},
   "source": [
    "Function to run against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a297b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running against test data\n",
    "def testingModel(w1,w2,b1,b2,x_test,y_test): \n",
    "    pred_test = np.where(sigmoid(sigmoid(x_test@w1+b1)@w2+b2) > 0.5,1,0)\n",
    "    return metrics.accuracy_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32ab7c",
   "metadata": {},
   "source": [
    "Testing each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d135633",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeOneTestAcc = testingModel(nodeOneW1, nodeOneW2, nodeOneB1, nodeOneB2, x_test1, y_test1)\n",
    "nodeTwoTestAcc = testingModel(nodeTwoW1, nodeTwoW2, nodeTwoB1, nodeTwoB2, x_test2, y_test2)\n",
    "fedNodeOneTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test1, y_test1)\n",
    "fedNodeTwoTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test2, y_test2)\n",
    "nonFedTotalAcc = testingModel(totalW1, totalW2, totalB1, totalB2, x_test, y_test)\n",
    "\n",
    "avIndividualAcc = (nodeOneTestAcc + nodeTwoTestAcc)/2\n",
    "avFedAcc = (fedNodeOneTestAcc + fedNodeOneTestAcc)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed09673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Non-Federated accuracy across both nodes (locally trained models): 0.429171668667467\n",
      "Average Federated accuracy across both nodes: 0.3877551020408163\n",
      "Non-Federated accuracy across both nodes(Globally trained models): 0.43\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Non-Federated accuracy across both nodes (locally trained models): {0}\\nAverage Federated accuracy across both nodes: {1}\\nNon-Federated accuracy across both nodes(Globally trained models): {2}\".format(avIndividualAcc, avFedAcc, nonFedTotalAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74456c0a",
   "metadata": {},
   "source": [
    "Not getting an improvement using federated learning. Let's Run this again with non-uniform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d31956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"Mixcancer.csv\")\n",
    "df = df.values\n",
    "x = df[:,1:]\n",
    "y = df[:,0]\n",
    "np.random.seed(42) \n",
    "x = preprocessing.MinMaxScaler().fit_transform(x)\n",
    "y=y.reshape(len(y),1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.5, random_state=0,shuffle = True)\n",
    "x_train1 = x_train[:int(0.5*x_train.shape[0]),:]\n",
    "x_train2 = x_train[int(0.5*x_train.shape[0]):,:]\n",
    "x_test1 = x_test[:int(0.5*x_test.shape[0]),:]\n",
    "x_test2 = x_test[int(0.5*x_test.shape[0]):,:]\n",
    "\n",
    "\n",
    "y_train = reshape(y_train)\n",
    "y_test = reshape(y_test)\n",
    "y_train1 = reshape(y_train[:int(0.5*y_train.shape[0]),:])\n",
    "y_test1 = reshape(y_test[:int(0.5*y_test.shape[0]),:])\n",
    "y_train2 = reshape(y_train[int(0.5*y_train.shape[0]):,:])\n",
    "y_test2 = reshape(y_test[int(0.5*y_test.shape[0]):,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8fef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Client One\n",
      "Training: Loss: 0.7038885874603892. Accuracy: 0.448\n",
      "Training on Client Two\n",
      "Training: Loss: 0.6995044898832302. Accuracy: 0.48\n",
      "Training on all data\n",
      "Training: Loss: 0.7016965386718098. Accuracy: 0.464\n",
      "Average Non-Federated accuracy across both nodes (locally trained models): 0.46399999999999997\n",
      "Average Federated accuracy across both nodes: 0.488\n",
      "Non-Federated accuracy across both nodes(Globally trained models): 0.464\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on Client One\")\n",
    "nodeOneW1,nodeOneW2,nodeOneB1,nodeOneB2 = DNN(x_train1,y_train1)\n",
    "print(\"Training on Client Two\")\n",
    "nodeTwoW1,nodeTwoW2,nodeTwoB1,nodeTwoB2 = DNN(x_train2,y_train2)\n",
    "print(\"Training on all data\")\n",
    "totalW1,totalW2,totalB1,totalB2 = DNN(x_train,y_train)\n",
    "\n",
    "globalW1, globalW2, globalB1, globalB2 = simpleFedAvg()\n",
    "\n",
    "nodeOneTestAcc = testingModel(nodeOneW1, nodeOneW2, nodeOneB1, nodeOneB2, x_test1, y_test1)\n",
    "nodeTwoTestAcc = testingModel(nodeTwoW1, nodeTwoW2, nodeTwoB1, nodeTwoB2, x_test2, y_test2)\n",
    "fedNodeOneTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test1, y_test1)\n",
    "fedNodeTwoTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test2, y_test2)\n",
    "nonFedTotalAcc = testingModel(totalW1, totalW2, totalB1, totalB2, x_test, y_test)\n",
    "\n",
    "avIndividualAcc = (nodeOneTestAcc + nodeTwoTestAcc)/2\n",
    "avFedAcc = (fedNodeOneTestAcc + fedNodeOneTestAcc)/2\n",
    "\n",
    "print(\"Average Non-Federated accuracy across both nodes (locally trained models): {0}\\nAverage Federated accuracy across both nodes: {1}\\nNon-Federated accuracy across both nodes(Globally trained models): {2}\".format(avIndividualAcc, avFedAcc, nonFedTotalAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d772b",
   "metadata": {},
   "source": [
    "Results still very close to maximum limit (if we view the total accuracy trained across the whole data-set as the maximum potential accuracy) so lets change up our approach so that we have an improvement when using federated learning, in order to achieve more interesting data. Getting greater accuracy than a global model is also unrealistic which is another reason to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d20e",
   "metadata": {},
   "source": [
    "To do this we are going to train a CNN model to evaluate the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcd529",
   "metadata": {},
   "source": [
    "Import tensorflow functionality and download CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762294b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad3b00",
   "metadata": {},
   "source": [
    "For experimentation, let's create a function to split our data into N clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b5638b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittingData(x_train,y_train,x_test,y_test,noClients):\n",
    "    if (len(x_train) % noClients != 0):\n",
    "        print(\"Data does not divide Equally\")\n",
    "        return 0,0,0,0\n",
    "    trainLen = len(x_train)\n",
    "    clientSize = trainLen/noClients\n",
    "    \n",
    "    x_train_clients = []\n",
    "    y_train_clients = []\n",
    "    x_test_clients = []\n",
    "    y_test_clients = []\n",
    "    \n",
    "    x_train_splits = np.split(x_train,noClients)\n",
    "    y_train_splits = np.split(y_train,noClients)\n",
    "    x_test_splits = np.split(x_test,noClients)\n",
    "    y_test_splits = np.split(y_test,noClients)\n",
    "    \n",
    "    for i in range(noClients):\n",
    "        x_train_clients.append(x_train_splits[i])\n",
    "        y_train_clients.append(y_train_splits[i])\n",
    "        x_test_clients.append(x_test_splits[i])\n",
    "        y_test_clients.append(y_test_splits[i])\n",
    "    \n",
    "    return x_train_clients, y_train_clients, x_test_clients, y_test_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "704372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numClients = 2\n",
    "x_train_clients, y_train_clients, x_test_clients, y_test_clients = splittingData(train_images, train_labels, test_images, test_labels, numClients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c8430",
   "metadata": {},
   "source": [
    "Creating a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8263a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "862f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModels(numClients):\n",
    "    models = []\n",
    "    for i in range(numClients):\n",
    "        print(\"Creating Model {0}\".format(i))\n",
    "        model = createModel()\n",
    "        model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "        model.fit(x_train_clients[i], y_train_clients[i], epochs=10, verbose=0)\n",
    "        models.append(model)\n",
    "    print(\"Creating Total Model\")\n",
    "    model = createModel()\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "    model.fit(train_images, train_labels, epochs=10, verbose=0)\n",
    "    \n",
    "    return models, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c76af31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model 0\n",
      "Creating Model 1\n",
      "Creating Model 2\n",
      "Creating Model 3\n",
      "Creating Model 4\n",
      "Creating Model 5\n",
      "Creating Model 6\n",
      "Creating Model 7\n",
      "Creating Total Model\n"
     ]
    }
   ],
   "source": [
    "models_, totalModel= trainModels(numClients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd6087c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels(numClients,models,totalMod):\n",
    "    localAccs = 0\n",
    "    globalAccs = 0\n",
    "    globalOnLocalAccs = 0\n",
    "    \n",
    "    print(\"Testing Local Models on Local Data\")\n",
    "    for i in range(numClients):\n",
    "        loss, acc = models[i].evaluate(x_test_clients[i],  y_test_clients[i], verbose=0)\n",
    "        localAccs += acc/numClients\n",
    "    \n",
    "    print(\"Testing Local Models On All Data\")\n",
    "    for i in range(numClients):\n",
    "        loss,acc = models[i].evaluate(test_images,  test_labels, verbose=0)\n",
    "        globalAccs += acc/numClients\n",
    "        \n",
    "    print(\"Testing Global model On Local Data\")\n",
    "    for i in range(numClients):\n",
    "        loss,acc = totalMod.evaluate(x_test_clients[i],  y_test_clients[i], verbose=0)\n",
    "        globalOnLocalAccs += acc/numClients\n",
    "    \n",
    "    print(\"Testing Global model On Global Data\")\n",
    "    totalLoss, totalAccTotal = totalModel.evaluate(test_images,  test_labels, verbose=0)\n",
    "        \n",
    "    return localAccs, globalAccs, globalOnLocalAccs, totalAccTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d6be356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Local Models on Local Data\n",
      "Testing Local Models On All Data\n",
      "Testing Global model On Local Data\n",
      "Testing Global model On Global Data\n",
      "Average accuracy using local models on local data  = 0.5461999997496605\n",
      "Average accuracy using local models on all data = 0.5419000089168549\n",
      "Average accuracy using all data to train a model on all data = 0.6777999997138977\n",
      "Average accuracy using all data to train a model on local data = 0.6777999997138977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avLocalAcc, avGlobalAcc, avGlobalOnLocalAcc,totalAccTotal = testModels(numClients,models_,totalModel)\n",
    "print(\"Average accuracy using local models on local data  = {0}\\nAverage accuracy using local models on all data = {1}\\nAverage accuracy using all data to train a model on all data = {2}\\nAverage accuracy using all data to train a model on local data = {3}\\n\".format(avLocalAcc, avGlobalAcc, totalAccTotal, avGlobalOnLocalAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6beacfa",
   "metadata": {},
   "source": [
    "Creating a blank CNN to hold the values of the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bef8da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createZeroWeightCNN():\n",
    "    layer1 = [np.zeros(shape = (3,3,3,32), dtype = 'float32'), np.zeros(shape = 32, dtype = 'float32')]\n",
    "    layer2 = [np.zeros(shape = (3,3,32,64), dtype = 'float32'), np.zeros(shape = 64, dtype = 'float32')]\n",
    "    layer3 = [np.zeros(shape = (10816,64), dtype = 'float32'), np.zeros(shape = (64), dtype = 'float32')]\n",
    "    layer4 = [np.zeros(shape = (64,10), dtype = 'float32'), np.zeros(shape = (10), dtype = 'float32')]\n",
    "\n",
    "    testModel = models.Sequential()\n",
    "    testModel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    testModel.add(layers.MaxPooling2D((2, 2)))\n",
    "    testModel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    testModel.add(layers.Flatten())\n",
    "    testModel.add(layers.Dense(64, activation='relu'))\n",
    "    testModel.add(layers.Dense(10))\n",
    "    \n",
    "    testModel.layers[0].set_weights(layer1)\n",
    "    testModel.layers[2].set_weights(layer2)\n",
    "    testModel.layers[4].set_weights(layer3)\n",
    "    testModel.layers[5].set_weights(layer4)\n",
    "    testModel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "    return testModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1fa0cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedAvg(models_):\n",
    "    layers_ = []\n",
    "    layers_.append(models_[0].layers[0].weights)\n",
    "    layers_.append(models_[0].layers[2].weights)\n",
    "    layers_.append(models_[0].layers[4].weights)\n",
    "    layers_.append(models_[0].layers[5].weights)\n",
    "\n",
    "    layersNums = [0,2,4,5]\n",
    "    for n in range(1,numClients):\n",
    "        count = 0\n",
    "        for i in layersNums:\n",
    "            for j in range(len(models_[0].layers[i].weights)):\n",
    "                layers_[count][j] = layers_[count][j] + (models_[n].layers[i].weights[j]/32)\n",
    "            count += 1\n",
    "    \n",
    "    fedModel = models.Sequential()\n",
    "    fedModel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "    fedModel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    fedModel.add(layers.MaxPooling2D((2, 2)))\n",
    "    fedModel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    fedModel.add(layers.Flatten())\n",
    "    fedModel.add(layers.Dense(64, activation='relu'))\n",
    "    fedModel.add(layers.Dense(10))\n",
    "\n",
    "    fedModel.layers[0].set_weights(layers_[0])\n",
    "    fedModel.layers[2].set_weights(layers_[1])\n",
    "    fedModel.layers[4].set_weights(layers_[2])\n",
    "    fedModel.layers[5].set_weights(layers_[3])\n",
    "    return fedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49cece50",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalModel = fedAvg(models_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4224af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Federated Model on Local Data\n",
      "40/40 - 0s - loss: 1.5876 - accuracy: 0.5144 - 363ms/epoch - 9ms/step\n",
      "40/40 - 0s - loss: 1.5069 - accuracy: 0.5328 - 172ms/epoch - 4ms/step\n",
      "40/40 - 0s - loss: 1.6279 - accuracy: 0.4952 - 163ms/epoch - 4ms/step\n",
      "40/40 - 0s - loss: 1.5139 - accuracy: 0.5424 - 171ms/epoch - 4ms/step\n",
      "40/40 - 0s - loss: 1.5913 - accuracy: 0.5168 - 167ms/epoch - 4ms/step\n",
      "40/40 - 0s - loss: 1.6642 - accuracy: 0.4872 - 179ms/epoch - 4ms/step\n",
      "40/40 - 0s - loss: 1.6939 - accuracy: 0.5016 - 165ms/epoch - 4ms/step\n",
      "40/40 - 0s - loss: 1.5996 - accuracy: 0.5232 - 171ms/epoch - 4ms/step\n",
      "0.5142000019550323\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Federated Model on Local Data\")\n",
    "fedAcc = 0\n",
    "for i in range(numClients):\n",
    "    loss, acc = globalModel.evaluate(x_test_clients[i],  y_test_clients[i], verbose=2)\n",
    "    fedAcc += acc/numClients\n",
    "print(fedAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187e629",
   "metadata": {},
   "source": [
    "This version of the federated learning paradigm updates the weights after each node has trained a model locally, now lets update the weights after every epoch and see the results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f0a0975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedAvgTwo(models_,fedModel):\n",
    "    layers_ = []\n",
    "    layers_.append(models_[0].layers[0].weights)\n",
    "    layers_.append(models_[0].layers[2].weights)\n",
    "    layers_.append(models_[0].layers[4].weights)\n",
    "    layers_.append(models_[0].layers[5].weights)\n",
    "    layersNums = [0,2,4,5]    \n",
    "    \n",
    "    for x in range(len(layersNums)):\n",
    "        for y in range(len(layers_[x])):\n",
    "            layers_[x][y] = layers_[x][y]/numClients\n",
    "\n",
    "    for n in range(1,numClients):\n",
    "        count = 0\n",
    "        for i in layersNums:\n",
    "            for j in range(len(models_[0].layers[i].weights)):\n",
    "                layers_[count][j] = layers_[count][j] + (models_[n].layers[i].weights[j]/numClients)\n",
    "            count += 1\n",
    "    count = 0\n",
    "    for i in layersNums:\n",
    "        for j in range(len(models_[0].layers[i].weights)):\n",
    "            layers_[count][j] = (layers_[count][j] + fedModel.layers[i].weights[j])/2\n",
    "        count += 1\n",
    "    fedModel.layers[0].set_weights(layers_[0])\n",
    "    fedModel.layers[2].set_weights(layers_[1])\n",
    "    fedModel.layers[4].set_weights(layers_[2])\n",
    "    fedModel.layers[5].set_weights(layers_[3])\n",
    "    return fedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7d9492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelsFed(numClients, noEpochs):\n",
    "    fedModel = createZeroWeightCNN()\n",
    "    models = []\n",
    "    for i in range(numClients):\n",
    "        model = createModel()\n",
    "        model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "        models.append(model)\n",
    "        \n",
    "    for ep in range(noEpochs):\n",
    "        for i in range(numClients):\n",
    "            models[i].fit(x_train_clients[i], y_train_clients[i], epochs=1, verbose=0)\n",
    "        fedModel = fedAvgTwo(models,fedModel)\n",
    "        testModelsFed(ep,fedModel)\n",
    "    return fedModel, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a1eee670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelsFed_(numClients, noEpochs):\n",
    "    fedModel = createZeroWeightCNN()\n",
    "    models = []\n",
    "    for i in range(numClients):\n",
    "        model = createModel()\n",
    "        model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "        models.append(model)\n",
    "        \n",
    "    for i in range(numClients):\n",
    "        models[i].fit(x_train_clients[i], y_train_clients[i], epochs=1, verbose=0)\n",
    "    fedModel = fedAvgTwo(models,fedModel)\n",
    "    testModelsFed(0,fedModel)\n",
    "    \n",
    "    for ep in range(1,noEpochs):\n",
    "        for i in range(numClients):\n",
    "            models[i] = fedModel\n",
    "            models[i].fit(x_train_clients[i], y_train_clients[i], epochs=1, verbose=0)\n",
    "        fedModel = fedAvgTwo(models,fedModel)\n",
    "        testModelsFed(ep,fedModel)\n",
    "\n",
    "    return fedModel, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dff511b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelsFed(ep,model):\n",
    "    fedAcc = 0\n",
    "    for i in range(numClients):\n",
    "        loss, acc = model.evaluate(x_test_clients[i],  y_test_clients[i], verbose=0)\n",
    "        fedAcc += acc/numClients\n",
    "    print(\"Accuracy at Epoch {0} is = {1}\".format(ep,fedAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e2216978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at Epoch 0 is = 0.1046999990940094\n",
      "Accuracy at Epoch 1 is = 0.5803999900817871\n",
      "Accuracy at Epoch 2 is = 0.612500011920929\n",
      "Accuracy at Epoch 3 is = 0.6521999835968018\n",
      "Accuracy at Epoch 4 is = 0.6742999851703644\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "os.environ['PYTHONHASHSEED']=str(2)\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(2)\n",
    "random.seed(2)\n",
    "noEpochs = 5\n",
    "fedModel5, finalLocaModels = trainModelsFed_(numClients, noEpochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc378fdd",
   "metadata": {},
   "source": [
    "The local nodes should start off with the basis of the global model not their own local model. First attempt is made at trainModelsFed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
