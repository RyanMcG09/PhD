{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ded2b11",
   "metadata": {},
   "source": [
    "# Initial Federated Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f34b9",
   "metadata": {},
   "source": [
    "This script creates a deep learning model for the detection of cancer given a number of features, this is then compared to a federated learning model. In the interest of fairness, the initial model is trained on the same amount of data as a single node in the federated learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c59dc",
   "metadata": {},
   "source": [
    "Importing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf3203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a57d6",
   "metadata": {},
   "source": [
    "Activation and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dda7ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation function\n",
    "def sigmoid(v):\n",
    "    return 1/(1+np.exp(-v))\n",
    "def sigmoid_der(v):\n",
    "    return sigmoid(v)*(1-sigmoid(v))\n",
    "\n",
    "#Loss Function\n",
    "def crossEntrop(o,y):\n",
    "    return (-y*(np.log(o)) - (1-y)* np.log(1-o))\n",
    "def crossEntrDeriv(o,y):\n",
    "    return -(y/o - (1-y)/(1-o))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc214611",
   "metadata": {},
   "source": [
    "Reading data and converting to two training and testing nodes for experimentation. We use the MinMaxScaler preprocessing to prevent overflow later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1740975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Mixcancer.csv\")\n",
    "df = df.values\n",
    "\n",
    "df[:,1:] = preprocessing.MinMaxScaler().fit_transform(df[:,1:])\n",
    "\n",
    "zeros = df[df[:,0] == 0]\n",
    "ones = df[df[:,0] == 1]\n",
    "\n",
    "no0sper = int(zeros.shape[0]/2)\n",
    "no1sper = int(ones.shape[0]/2)\n",
    "\n",
    "node1 = np.concatenate([zeros[:int(0.4*zeros.shape[0]),:], ones[:int(0.4*ones.shape[0]),:]])\n",
    "node2 = np.concatenate([zeros[int(0.4*zeros.shape[0]):int(0.8*zeros.shape[0]),:], ones[int(0.4*ones.shape[0]):int(0.8*ones.shape[0]),:]])\n",
    "test1 = np.concatenate([zeros[int(0.8*zeros.shape[0]):int(0.9*zeros.shape[0]),:], ones[int(0.8*ones.shape[0]):int(0.9*ones.shape[0]),:]])\n",
    "test2 = np.concatenate([zeros[int(0.9*zeros.shape[0]):,:], ones[int(0.9*ones.shape[0]):,:]])\n",
    "fullTrain = np.concatenate([zeros[:int(0.8*zeros.shape[0]),:], ones[:int(0.8*ones.shape[0]),:]])\n",
    "fullTest = np.concatenate([zeros[int(0.8*zeros.shape[0]):,:], ones[int(0.8*ones.shape[0]):,:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a0de2",
   "metadata": {},
   "source": [
    "Splitting the nodes training and test data into features and target feature. And reshaping acitvation feature for use in DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82541166",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = node1[:,1:]\n",
    "y_train1 = node1[:,0]\n",
    "x_train2 = node2[:,1:]\n",
    "y_train2 = node2[:,0]\n",
    "\n",
    "x_test1 = test1[:,1:]\n",
    "y_test1 = test1[:,0]\n",
    "x_test2 = test2[:,1:]\n",
    "y_test2 = test2[:,0]\n",
    "\n",
    "x_train = fullTrain[:,1:]\n",
    "y_train = fullTrain[:,0]\n",
    "x_test = fullTest[:,1:]\n",
    "y_test = fullTest[:,0]\n",
    "\n",
    "def reshape(data):\n",
    "    data = data.reshape(len(data),1)\n",
    "    return data\n",
    "\n",
    "y_train = reshape(y_train)\n",
    "y_test = reshape(y_test)\n",
    "y_train1 = reshape(y_train1)\n",
    "y_test1 = reshape(y_test1)\n",
    "y_train2 = reshape(y_train2)\n",
    "y_test2 = reshape(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83a5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Altering the DNN algorithm to train two individual models\n",
    "def DNN(x_train,y_train):\n",
    "    np.random.seed(42)\n",
    "    neuronNo = 5\n",
    "    w1 = np.random.uniform(-1,1,[len(x_train[0]),neuronNo])\n",
    "    w2 = np.random.uniform(-1,1,[neuronNo,1])\n",
    "    b1 = np.zeros([1,neuronNo])\n",
    "    b2 = np.zeros([1,1])\n",
    "    l = 0.01\n",
    "    epochs = 1\n",
    "    miniBatch = 400\n",
    "\n",
    "    train_L = []\n",
    "    test_L = []\n",
    "    train_Acc = []\n",
    "    test_Acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(x_train[0]), miniBatch):\n",
    "\n",
    "            x_trainSample = x_train[i:i+miniBatch,:]\n",
    "            y_trainSample = y_train[i:i+miniBatch,:]\n",
    "\n",
    "            #feedforward\n",
    "            in1 = x_trainSample@w1 + b1\n",
    "            o1 = sigmoid(in1)\n",
    "            in2 = o1@w2 + b2\n",
    "            o2 = sigmoid(in2)\n",
    "\n",
    "            #backpropagation output layer\n",
    "            dE_dO2 = crossEntrDeriv(o2, y_trainSample)\n",
    "            dO2_dIn2 = sigmoid_der(in2)\n",
    "            dIn2_dW2 = o1\n",
    "            dIn2_B2 = 1\n",
    "            dE_dW2 = (1/miniBatch)*dIn2_dW2.T@(dE_dO2*dO2_dIn2)\n",
    "            dE_dB2 = (1/miniBatch)*np.ones([1,len(x_trainSample)])@(dE_dO2*dO2_dIn2)\n",
    "\n",
    "            #backpropagation hidden layer\n",
    "            dIn2_dO1 = w2\n",
    "            dO1_dIn1 = sigmoid_der(in1)\n",
    "            dIn1_dW1 = x_trainSample\n",
    "            dE_dW1 = (1/miniBatch)*dIn1_dW1.T@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "            dE_dB1 = (1/miniBatch)*np.ones([len(x_trainSample)])@((dE_dO2*dO2_dIn2@dIn2_dO1.T)*dO1_dIn1)\n",
    "\n",
    "            #updating parameters\n",
    "            b2-=l*dE_dB2\n",
    "            w2-=l*dE_dW2\n",
    "            b1-=l*dE_dB1\n",
    "            w1-=l*dE_dW1\n",
    "\n",
    "        #Error\n",
    "        error = crossEntrop(o2 ,y_trainSample).mean()\n",
    "        train_L.append(error)\n",
    "\n",
    "        #Accuracy\n",
    "        pred_train = np.where(o2 > 0.5, 1,0)\n",
    "        train_Acc.append(metrics.accuracy_score(y_trainSample,pred_train))\n",
    "\n",
    "    #print(\"Training: Loss: {0}. Accuracy: {1}. Error: {2}\".format(train_L[-1],train_Acc[-1], 1 - train_Acc[-1]))\n",
    "    print(\"Training: Loss: {0}. Accuracy: {1}\".format(train_L[-1],train_Acc[-1]))\n",
    "    \n",
    "    return w1,w2,b1,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd53544",
   "metadata": {},
   "source": [
    "Training The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7facecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Client One\n",
      "Training: Loss: 0.7145093617748137. Accuracy: 0.44\n",
      "Training on Client Two\n",
      "Training: Loss: 0.6985157381751381. Accuracy: 0.505\n",
      "Training on all data\n",
      "Training: Loss: 0.7065125499749757. Accuracy: 0.4725\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on Client One\")\n",
    "nodeOneW1,nodeOneW2,nodeOneB1,nodeOneB2 = DNN(x_train1,y_train1)\n",
    "print(\"Training on Client Two\")\n",
    "nodeTwoW1,nodeTwoW2,nodeTwoB1,nodeTwoB2 = DNN(x_train2,y_train2)\n",
    "print(\"Training on all data\")\n",
    "totalW1,totalW2,totalB1,totalB2 = DNN(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a07eb",
   "metadata": {},
   "source": [
    "We now have the weights and biases of two models trained individually, lets perform aggregation and see how our new model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccada435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleFedAvg():\n",
    "    globalW1 = (nodeOneW1 + nodeOneW1)/2\n",
    "    globalW2 = (nodeOneW2 + nodeOneW2)/2\n",
    "    globalB1 = (nodeOneB1 + nodeOneB1)/2\n",
    "    globalB2 = (nodeOneB2 + nodeOneB2)/2\n",
    "    return globalW1, globalW2, globalB1, globalB2\n",
    "    \n",
    "globalW1, globalW2, globalB1, globalB2 = simpleFedAvg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0bea8",
   "metadata": {},
   "source": [
    "Function to run against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a297b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running against test data\n",
    "def testingModel(w1,w2,b1,b2,x_test,y_test): \n",
    "    pred_test = np.where(sigmoid(sigmoid(x_test@w1+b1)@w2+b2) > 0.5,1,0)\n",
    "    return metrics.accuracy_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32ab7c",
   "metadata": {},
   "source": [
    "Testing each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d135633",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeOneTestAcc = testingModel(nodeOneW1, nodeOneW2, nodeOneB1, nodeOneB2, x_test1, y_test1)\n",
    "nodeTwoTestAcc = testingModel(nodeTwoW1, nodeTwoW2, nodeTwoB1, nodeTwoB2, x_test2, y_test2)\n",
    "fedNodeOneTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test1, y_test1)\n",
    "fedNodeTwoTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test2, y_test2)\n",
    "nonFedTotalAcc = testingModel(totalW1, totalW2, totalB1, totalB2, x_test, y_test)\n",
    "\n",
    "avIndividualAcc = (nodeOneTestAcc + nodeTwoTestAcc)/2\n",
    "avFedAcc = (fedNodeOneTestAcc + fedNodeOneTestAcc)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aed09673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Non-Federated accuracy across both nodes (locally trained models): 0.429171668667467\n",
      "Average Federated accuracy across both nodes: 0.3877551020408163\n",
      "Non-Federated accuracy across both nodes(Globally trained models): 0.43\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Non-Federated accuracy across both nodes (locally trained models): {0}\\nAverage Federated accuracy across both nodes: {1}\\nNon-Federated accuracy across both nodes(Globally trained models): {2}\".format(avIndividualAcc, avFedAcc, nonFedTotalAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74456c0a",
   "metadata": {},
   "source": [
    "Not getting an improvement using federated learning. Let's Run this again with non-uniform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49d31956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"Mixcancer.csv\")\n",
    "df = df.values\n",
    "x = df[:,1:]\n",
    "y = df[:,0]\n",
    "np.random.seed(42) \n",
    "x = preprocessing.MinMaxScaler().fit_transform(x)\n",
    "y=y.reshape(len(y),1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.5, random_state=0,shuffle = True)\n",
    "x_train1 = x_train[:int(0.5*x_train.shape[0]),:]\n",
    "x_train2 = x_train[int(0.5*x_train.shape[0]):,:]\n",
    "x_test1 = x_test[:int(0.5*x_test.shape[0]),:]\n",
    "x_test2 = x_test[int(0.5*x_test.shape[0]):,:]\n",
    "\n",
    "\n",
    "y_train = reshape(y_train)\n",
    "y_test = reshape(y_test)\n",
    "y_train1 = reshape(y_train[:int(0.5*y_train.shape[0]),:])\n",
    "y_test1 = reshape(y_test[:int(0.5*y_test.shape[0]),:])\n",
    "y_train2 = reshape(y_train[int(0.5*y_train.shape[0]):,:])\n",
    "y_test2 = reshape(y_test[int(0.5*y_test.shape[0]):,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da8fef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Client One\n",
      "Training: Loss: 0.7038885874603892. Accuracy: 0.448\n",
      "Training on Client Two\n",
      "Training: Loss: 0.6995044898832302. Accuracy: 0.48\n",
      "Training on all data\n",
      "Training: Loss: 0.7016965386718098. Accuracy: 0.464\n",
      "Average Non-Federated accuracy across both nodes (locally trained models): 0.46399999999999997\n",
      "Average Federated accuracy across both nodes: 0.488\n",
      "Non-Federated accuracy across both nodes(Globally trained models): 0.464\n"
     ]
    }
   ],
   "source": [
    "print(\"Training on Client One\")\n",
    "nodeOneW1,nodeOneW2,nodeOneB1,nodeOneB2 = DNN(x_train1,y_train1)\n",
    "print(\"Training on Client Two\")\n",
    "nodeTwoW1,nodeTwoW2,nodeTwoB1,nodeTwoB2 = DNN(x_train2,y_train2)\n",
    "print(\"Training on all data\")\n",
    "totalW1,totalW2,totalB1,totalB2 = DNN(x_train,y_train)\n",
    "\n",
    "globalW1, globalW2, globalB1, globalB2 = simpleFedAvg()\n",
    "\n",
    "nodeOneTestAcc = testingModel(nodeOneW1, nodeOneW2, nodeOneB1, nodeOneB2, x_test1, y_test1)\n",
    "nodeTwoTestAcc = testingModel(nodeTwoW1, nodeTwoW2, nodeTwoB1, nodeTwoB2, x_test2, y_test2)\n",
    "fedNodeOneTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test1, y_test1)\n",
    "fedNodeTwoTestAcc = testingModel(globalW1, globalW2, globalB1, globalB2, x_test2, y_test2)\n",
    "nonFedTotalAcc = testingModel(totalW1, totalW2, totalB1, totalB2, x_test, y_test)\n",
    "\n",
    "avIndividualAcc = (nodeOneTestAcc + nodeTwoTestAcc)/2\n",
    "avFedAcc = (fedNodeOneTestAcc + fedNodeOneTestAcc)/2\n",
    "\n",
    "print(\"Average Non-Federated accuracy across both nodes (locally trained models): {0}\\nAverage Federated accuracy across both nodes: {1}\\nNon-Federated accuracy across both nodes(Globally trained models): {2}\".format(avIndividualAcc, avFedAcc, nonFedTotalAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d772b",
   "metadata": {},
   "source": [
    "Results still very close to maximum limit (if we view the total accuracy trained across the whole data-set as the maximum potential accuracy) so lets change up our approach so that we have an improvement when using federated learning, in order to achieve more interesting data. Getting greater accuracy than a global model is also unrealistic which is another reason to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d20e",
   "metadata": {},
   "source": [
    "To do this we are going to train a CNN model to evaluate the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcd529",
   "metadata": {},
   "source": [
    "Import tensorflow functionality and download CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "762294b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad3b00",
   "metadata": {},
   "source": [
    "For experimentation, let's create a function to split our data into N clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b5638b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittingData(x_train,y_train,x_test,y_test,noClients):\n",
    "    if (len(x_train) % noClients != 0):\n",
    "        print(\"Data does not divide Equally\")\n",
    "        return 0,0,0,0\n",
    "    trainLen = len(x_train)\n",
    "    clientSize = trainLen/noClients\n",
    "    \n",
    "    x_train_clients = []\n",
    "    y_train_clients = []\n",
    "    x_test_clients = []\n",
    "    y_test_clients = []\n",
    "    \n",
    "    x_train_splits = np.split(x_train,noClients)\n",
    "    y_train_splits = np.split(y_train,noClients)\n",
    "    x_test_splits = np.split(x_test,noClients)\n",
    "    y_test_splits = np.split(y_test,noClients)\n",
    "    \n",
    "    for i in range(noClients):\n",
    "        x_train_clients.append(x_train_splits[i])\n",
    "        y_train_clients.append(y_train_splits[i])\n",
    "        x_test_clients.append(x_test_splits[i])\n",
    "        y_test_clients.append(y_test_splits[i])\n",
    "    \n",
    "    return x_train_clients, y_train_clients, x_test_clients, y_test_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "704372e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numClients = 8\n",
    "x_train_clients, y_train_clients, x_test_clients, y_test_clients = splittingData(train_images, train_labels, test_images, test_labels, numClients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ef640",
   "metadata": {},
   "source": [
    "Creating a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8263a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "862f421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModels(numClients):\n",
    "    models = []\n",
    "    for i in range(numClients):\n",
    "        print(\"Creating Model {0}\".format(i))\n",
    "        model = createModel()\n",
    "        model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "        model.fit(x_train_clients[i], y_train_clients[i], epochs=1, verbose=0)\n",
    "        models.append(model)\n",
    "    print(\"Creating Total Model\")\n",
    "    model = createModel()\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "    model.fit(train_images, train_labels, epochs=1, verbose=0)\n",
    "    \n",
    "    return models, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c76af31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model 0\n",
      "Creating Model 1\n",
      "Creating Model 2\n",
      "Creating Model 3\n",
      "Creating Model 4\n",
      "Creating Model 5\n",
      "Creating Model 6\n",
      "Creating Model 7\n",
      "Creating Total Model\n"
     ]
    }
   ],
   "source": [
    "models_, totalModel= trainModels(numClients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd6087c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels(numClients,models,totalMod):\n",
    "    localAccs = 0\n",
    "    globalAccs = 0\n",
    "    globalOnLocalAccs = 0\n",
    "    \n",
    "    print(\"Testing Local Models on Local Data\")\n",
    "    for i in range(numClients):\n",
    "        loss, acc = models[i].evaluate(x_test_clients[i],  y_test_clients[i], verbose=0)\n",
    "        localAccs += acc/numClients\n",
    "    \n",
    "    print(\"Testing Local Models On All Data\")\n",
    "    for i in range(numClients):\n",
    "        loss,acc = models[i].evaluate(test_images,  test_labels, verbose=0)\n",
    "        globalAccs += acc/numClients\n",
    "        \n",
    "    print(\"Testing Global model On Local Data\")\n",
    "    for i in range(numClients):\n",
    "        loss,acc = totalMod.evaluate(x_test_clients[i],  y_test_clients[i], verbose=0)\n",
    "        globalOnLocalAccs += acc/numClients\n",
    "    \n",
    "    print(\"Testing Global model On Global Data\")\n",
    "    totalLoss, totalAccTotal = totalModel.evaluate(test_images,  test_labels, verbose=0)\n",
    "        \n",
    "    return localAccs, globalAccs, globalOnLocalAccs, totalAccTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d6be356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing On Local Models on Local Data\n",
      "Testing Local Models On All Data\n",
      "Testing Global model On Local Data\n",
      "Testing Global model On Global Data\n",
      "Average accuracy using local models on local data  = 0.3911999985575676\n",
      "Average accuracy using local models on all data = 0.3934124931693077\n",
      "Average accuracy using all data to train a model on all data = 0.6021000146865845\n",
      "Average accuracy using all data to train a model on local data = 0.6020999997854233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avLocalAcc, avGlobalAcc, avGlobalOnLocalAcc,totalAccTotal = testModels(numClients,models_,totalModel)\n",
    "print(\"Average accuracy using local models on local data  = {0}\\nAverage accuracy using local models on all data = {1}\\nAverage accuracy using all data to train a model on all data = {2}\\nAverage accuracy using all data to train a model on local data = {3}\\n\".format(avLocalAcc, avGlobalAcc, totalAccTotal, avGlobalOnLocalAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37b1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
